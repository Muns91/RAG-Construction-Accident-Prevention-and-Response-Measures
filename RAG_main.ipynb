{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a7b12-aa41-452b-b40c-c03ac3c218d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pypdf\n",
    "!pip install -U langchain-community\n",
    "!pip install sentence-transformers\n",
    "!pip install -U bitsandbytes\n",
    "!pip install PyPDF2\n",
    "!pip install numpy==1.23.5\n",
    "!pip install pandas\n",
    "!pip install -U langchain-huggingface\n",
    "!pip install accelerate>=0.26.0\n",
    "!pip install fitz\n",
    "!pip install frontend\n",
    "!pip install pdfplumber\n",
    "!pip install langchain PyPDFium2\n",
    "!pip install faiss-gpu\n",
    "!pip install grandalf\n",
    "!pip install rank_bm25\n",
    "!pip install tiktoken\n",
    "!pip install -qU langchain-teddynote\n",
    "!pip install matplotlib\n",
    "!pip install chromadb\n",
    "!pip install --upgrade langchain pydantic\n",
    "!pip install pymupdf4llm\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b1644-9c6a-4a66-a3bd-720f77605b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import PyPDFium2Loader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from transformers import pipeline\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import pymupdf4llm\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_teddynote.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "import pdfplumber\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5c271-8534-4bf5-9dae-6d340c807562",
   "metadata": {},
   "source": [
    "# Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70224ba1-a077-4185-b84f-ef7fb1f004e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_Embedding_model = \"jhgan/ko-sbert-nli\"\n",
    "# BAAI/bge-m3\n",
    "\n",
    "TEST_Embedding = \"jhgan/ko-sbert-sts\"\n",
    "LLM_Model =\"CarrotAI/Llama-3.2-Rabbit-Ko-3B-Instruct-2412\"\n",
    "# \"beomi/OPEN-SOLAR-KO-10.7B\"\n",
    "# \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\"\n",
    "\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Text splitter\n",
    "N_Chunk_size = 1000\n",
    "N_Chunk_overlab = 100\n",
    "\n",
    "R_k = 3\n",
    "\n",
    "# Ensemble Retirever\n",
    "# 1. CSV\n",
    "# 2. PDF\n",
    "Ensemble_rate = [0.5, 0.5] \n",
    "\n",
    "# Sample Test\n",
    "Max_token_length = 100\n",
    "\n",
    "# EmbeddingsFilter\n",
    "# threshold = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fa4a4-ad5f-496b-8a48-5209e9c042d0",
   "metadata": {},
   "source": [
    "# 1. Load PDF\n",
    "## Preprocess PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521dbce2-614e-4872-b363-9f72409201f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # ì¤„ë°”ê¿ˆ ì œê±°\n",
    "    #text = text.replace(\"\\n\", \" \")  \n",
    "    # 'KOSHA GUIDE' ì œê±°\n",
    "    text = text.replace(\"KOSHA GUIDE\", \"\")  \n",
    "    # 4ìë¦¬ ìˆ«ì(ë…„ë„) ì œê±°\n",
    "    text = re.sub(r\"\\b\\d{4}\\b\", \"\", text)\n",
    "    # [ì°¸ê³ 'ìˆ«ì'] í˜•íƒœ ì œê±°\n",
    "    text = re.sub(r\"\\[ì°¸ê³ '\\d+'\\]\", \"\", text)\n",
    "    # '[ê¸€ì]' í˜•íƒœ ì œê±°\n",
    "    text = re.sub(r\"'\\w+'\", \"\", text)\n",
    "    # '- ìˆ«ì -' í˜•íƒœ ì œê±°\n",
    "    text = re.sub(r\"-\\s*\\d+\\s*-\", \"\", text)\n",
    "    # '<ì‚¬ì§„ ìˆ«ì>' í˜•íƒœ ì œê±°\n",
    "    text = re.sub(r\"<(ì‚¬ì§„|ê·¸ë¦¼|í‘œ)\\s*\\d+>|(ì‚¬ì§„|ê·¸ë¦¼|í‘œ)\\s*\\d+\", \"\", text)\n",
    "    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  \n",
    "    return text\n",
    "\n",
    "def load_pdfs_from_folder(folder_path):\n",
    "    documents = []\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    \n",
    "    # PDF íŒŒì¼ ë¡œë“œ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "    for file_name in tqdm(pdf_files, desc=\"PDF íŒŒì¼ ë¡œë“œ ì¤‘\", unit=\"íŒŒì¼\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            # ê° í˜ì´ì§€ ë¡œë“œ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "            for i in tqdm(range(len(pdf.pages)), desc=f\"{file_name} í˜ì´ì§€ ë¡œë“œ ì¤‘\", unit=\"í˜ì´ì§€\"):\n",
    "                page = pdf.pages[i]\n",
    "                # í‘œì™€ ì´ë¯¸ì§€ë¥¼ ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    # ì „ì²˜ë¦¬ ì ìš©\n",
    "                    cleaned_text = clean_text(text)\n",
    "                    if cleaned_text:  # ì „ì²˜ë¦¬ í›„ì—ë„ í…ìŠ¤íŠ¸ê°€ ë‚¨ì•„ìˆë‹¤ë©´ ì¶”ê°€\n",
    "                        documents.append({\n",
    "                            \"page_content\": cleaned_text,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": file_path,\n",
    "                                \"page\": i + 1\n",
    "                            }\n",
    "                        })\n",
    "    return documents\n",
    "\n",
    "# PDF ë¬¸ì„œ ë¡œë“œ\n",
    "pdf_docs = load_pdfs_from_folder(\"ê±´ì„¤ì•ˆì „ì§€ì¹¨/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a576f33-8365-429d-a4be-b9f5ab7d1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "Page_num = 150\n",
    "print(pdf_docs[Page_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ad692-ea77-44b3-a838-c260a2048b7b",
   "metadata": {},
   "source": [
    "# 2. Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38435bee-6608-42e3-88f2-0d9fe0428e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    page_lengths = []\n",
    "\n",
    "    for doc in documents:\n",
    "        splits = text_splitter.split_text(doc[\"page_content\"])\n",
    "\n",
    "        page_length = len(doc[\"page_content\"])\n",
    "        page_lengths.append({\n",
    "            \"source\": doc[\"metadata\"][\"source\"],\n",
    "            \"page\": doc[\"metadata\"][\"page\"],\n",
    "            \"page_length\": page_length\n",
    "        })\n",
    "\n",
    "        for split in splits:\n",
    "            chunks.append({\n",
    "                \"source\": doc[\"metadata\"][\"source\"],\n",
    "                \"page_number\": doc[\"metadata\"][\"page\"],\n",
    "                \"text\": split\n",
    "            })\n",
    "\n",
    "    return chunks, page_lengths\n",
    "\n",
    "# PDF ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ë¶„í•  ë° í˜ì´ì§€ ê¸¸ì´ í™•ì¸\n",
    "pdf_chunks, page_lengths = split_documents(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b659c0-14e2-456d-abf1-38b7364bd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunks[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede26ea-9115-410d-9eab-99ee01c98324",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_lengths_values = [p['page_length'] for p in page_lengths]\n",
    "\n",
    "# ìµœëŒ€, ìµœì†Œ, í‰ê·  ê¸¸ì´ ì¶œë ¥\n",
    "max_length = max(page_lengths_values)\n",
    "min_length = min(page_lengths_values)\n",
    "avg_length = np.mean(page_lengths_values)\n",
    "\n",
    "print(f\"ğŸ“ ìµœëŒ€ í˜ì´ì§€ ê¸¸ì´: {max_length}\")\n",
    "print(f\"ğŸ“ ìµœì†Œ í˜ì´ì§€ ê¸¸ì´: {min_length}\")\n",
    "print(f\"ğŸ“ í‰ê·  í˜ì´ì§€ ê¸¸ì´: {avg_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e3852-aa48-440b-a7ff-2ff4e7ba2ce1",
   "metadata": {},
   "source": [
    "# Reduce Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3f99d-b74c-402e-9a25-c59f5dfab61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_lengthsì—ì„œ ì¸ë±ìŠ¤ì™€ page_length ì¶”ì¶œ\n",
    "page_lengths_values_with_index = [(index, p['page_length']) for index, p in enumerate(page_lengths)]\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "df_page_lengths = pd.DataFrame(page_lengths_values_with_index, columns=[\"Index\", \"Page Length\"])\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "df_page_lengths_filtered = df_page_lengths[df_page_lengths[\"Page Length\"] <= 50]\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "df_page_lengths_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac297d0-7de0-4ffd-ab79-348919be8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins with intervals of 100\n",
    "bin_intervals = range(0, max_length + 100, 50)\n",
    "\n",
    "# Plot the histogram with the new bin intervals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(page_lengths_values, bins=bin_intervals, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Distribution of PDF Page Lengths\")\n",
    "plt.xlabel(\"Page Length (Number of Characters)\")\n",
    "plt.ylabel(\"Number of Pages\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the counts for each bin\n",
    "counts, bin_edges = np.histogram(page_lengths_values, bins=bin_intervals)\n",
    "\n",
    "# Display the bin ranges and corresponding counts\n",
    "for i in range(len(counts)):\n",
    "    print(f\"Bin {bin_edges[i]} - {bin_edges[i+1]}: {counts[i]} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e23bd-064c-417c-b6df-9c3e59a47d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunks[808]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f943a-d3ab-4b7b-a536-3640a883bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# 'text'ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°\n",
    "text_lengths = [len(chunk['text']) for chunk in pdf_chunks]\n",
    "\n",
    "# ë¶„í¬ë„ ê·¸ë¦¬ê¸°\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(text_lengths, bins=30, kde=True)\n",
    "plt.title('PDF Chunks Text Length Distribution')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175ff00-569d-4008-9e49-a1207fb1ec57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0ì—ì„œ 1000 ì‚¬ì´ì˜ ê¸¸ì´ë¥¼ ê°€ì§„ ìš”ì†Œì˜ ìˆ˜ í™•ì¸\n",
    "filtered_lengths = [length for length in text_lengths if 0 <= length <= 1000]\n",
    "\n",
    "# êµ¬ê°„ ì„¤ì •\n",
    "bin_intervals = range(0, 1050, 50)\n",
    "\n",
    "# ê° êµ¬ê°„ì˜ ê°œìˆ˜ ê³„ì‚°\n",
    "counts, bin_edges = np.histogram(filtered_lengths, bins=bin_intervals)\n",
    "\n",
    "# ê° êµ¬ê°„ì˜ ë²”ìœ„ì™€ ê°œìˆ˜ ì¶œë ¥\n",
    "for i in range(len(counts)):\n",
    "    print(f\"Bin {bin_edges[i]} - {bin_edges[i+1]}: {counts[i]} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bab77b-e87c-4dc6-8602-9843b8feb3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bin 0 - 100ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "bin_0_to_100_indices = [i for i, length in enumerate(filtered_lengths) if 0 <= length < 100]\n",
    "\n",
    "# Bin 0 - 100ì— í•´ë‹¹í•˜ëŠ” ìš”ì†Œ ì œê±°\n",
    "filtered_lengths = [length for i, length in enumerate(filtered_lengths) if i not in bin_0_to_100_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518c933-ab75-40c5-acc4-a40952e4be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œê±° í›„ ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ì œê±° í›„ filtered_lengthsì˜ ê¸¸ì´:\", len(filtered_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b18b59-dcd5-48d3-830a-655af0ad6c9a",
   "metadata": {},
   "source": [
    "# â˜… RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34813c8-a175-4ccc-bd59-6cb9a2de879a",
   "metadata": {},
   "source": [
    "# 2. CSV Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407651f-4fd9-4ffd-bac2-3574c9813032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ë°ì´í„° ì „ì²˜ë¦¬ (Train & Test ë°ì´í„°)\n",
    "train = pd.read_csv('./train.csv', encoding='utf-8-sig')\n",
    "test = pd.read_csv('./test.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a7468-6056-42b1-8678-e2823a3439e5",
   "metadata": {},
   "source": [
    "## 2-1 Column Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237df317-0151-4f9c-99be-ea72bd55967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠ¹ì • ì»¬ëŸ¼ ì œê±°\n",
    "columns_to_drop = ['ë°œìƒì¼ì‹œ', 'ì‚¬ê³ ì¸ì§€ ì‹œê°„', 'ë‚ ì”¨', 'ê¸°ì˜¨', 'ìŠµë„', 'ì—°ë©´ì ', 'ì¸µ ì •ë³´']\n",
    "\n",
    "# train ë°ì´í„°ì—ì„œ ì»¬ëŸ¼ ì œê±°\n",
    "train = train.drop(columns=columns_to_drop)\n",
    "\n",
    "# test ë°ì´í„°ì—ì„œ ì»¬ëŸ¼ ì œê±°\n",
    "test = test.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ade6ff-de7f-4ff6-88dd-4d9db1289c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ìš°ê¸°)\n",
    "train['ê³µì‚¬ì¢…ë¥˜'] = train['ê³µì‚¬ì¢…ë¥˜'].fillna('')\n",
    "train['ê³µì¢…'] = train['ê³µì¢…'].fillna('')\n",
    "train['ì‚¬ê³ ê°ì²´'] = train['ì‚¬ê³ ê°ì²´'].fillna('')\n",
    "train['ì‘ì—…í”„ë¡œì„¸ìŠ¤'] = train['ì‘ì—…í”„ë¡œì„¸ìŠ¤'].fillna('')\n",
    "train['ì‚¬ê³ ì›ì¸'] = train['ì‚¬ê³ ì›ì¸'].fillna('')\n",
    "\n",
    "test['ê³µì‚¬ì¢…ë¥˜'] = test['ê³µì‚¬ì¢…ë¥˜'].fillna('')\n",
    "test['ê³µì¢…'] = test['ê³µì¢…'].fillna('')\n",
    "test['ì‚¬ê³ ê°ì²´'] = test['ì‚¬ê³ ê°ì²´'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05856fb-ce9b-47d3-b34c-4e5678206d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬\n",
    "train['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)'] = train['ê³µì‚¬ì¢…ë¥˜'].str.split(' / ').str.get(0)\n",
    "train['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)'] = train['ê³µì‚¬ì¢…ë¥˜'].str.split(' / ').str.get(1)\n",
    "\n",
    "train['ê³µì¢…(ëŒ€ë¶„ë¥˜)'] = train['ê³µì¢…'].str.split(' > ').str[0]\n",
    "train['ê³µì¢…(ì¤‘ë¶„ë¥˜)'] = train['ê³µì¢…'].str.split(' > ').str[1]\n",
    "train['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)'] = train['ì‚¬ê³ ê°ì²´'].str.split(' > ').str[0]\n",
    "train['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)'] = train['ì‚¬ê³ ê°ì²´'].str.split(' > ').str[1]\n",
    "\n",
    "test['ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)'] = test['ê³µì‚¬ì¢…ë¥˜'].str.split(' / ').str[0]\n",
    "test['ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)'] = test['ê³µì‚¬ì¢…ë¥˜'].str.split(' / ').str[1]\n",
    "test['ê³µì¢…(ëŒ€ë¶„ë¥˜)'] = test['ê³µì¢…'].str.split(' > ').str[0]\n",
    "test['ê³µì¢…(ì¤‘ë¶„ë¥˜)'] = test['ê³µì¢…'].str.split(' > ').str[1]\n",
    "test['ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)'] = test['ì‚¬ê³ ê°ì²´'].str.split(' > ').str[0]\n",
    "test['ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)'] = test['ì‚¬ê³ ê°ì²´'].str.split(' > ').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1fa97-dcaa-4b35-9274-fa85249f4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ë°ì´í„° í†µí•© ìƒì„±\n",
    "combined_training_data = train.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"'{row['ì‘ì—…í”„ë¡œì„¸ìŠ¤']}', '{row['ì¸ì ì‚¬ê³ ']}', '{row['ì‚¬ê³ ì›ì¸']}' \"\n",
    "        ),\n",
    "        \"answer\": row[\"ì¬ë°œë°©ì§€ëŒ€ì±… ë° í–¥í›„ì¡°ì¹˜ê³„íš\"]\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "combined_training_data = pd.DataFrame(list(combined_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e5645-3ddc-4f38-b53a-8849d8a99b10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_training_data = pd.concat([train['ID'], combined_training_data], axis=1)\n",
    "combined_training_data['Type'] = 'csv'\n",
    "combined_training_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee9c59-e185-4ce9-9d3d-4685f2c4b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = [\n",
    "    f\"Q: {q1}\\nA: {a1}\" \n",
    "    for q1, a1, in zip(combined_training_data['question'], combined_training_data['answer'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a9471-0005-457f-a962-0c9e1bf71968",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f846aaa-a586-4991-979f-fffa5d5f6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train_documentsì˜ ê° ìš”ì†Œì˜ ê¸¸ì´ ê³„ì‚°\n",
    "lengths = [len(doc) for doc in train_documents]\n",
    "\n",
    "# ìµœëŒ€, ìµœì†Œ, í‰ê·  ê³„ì‚°\n",
    "max_length = np.max(lengths)\n",
    "min_length = np.min(lengths)\n",
    "mean_length = np.mean(lengths)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ìµœëŒ€ ê¸¸ì´: {max_length}\")\n",
    "print(f\"ìµœì†Œ ê¸¸ì´: {min_length}\")\n",
    "print(f\"í‰ê·  ê¸¸ì´: {mean_length:.2f}\")\n",
    "\n",
    "# íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lengths, bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Length of train_documents')\n",
    "plt.xlabel('Length of Document')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f9cf8-a9ef-43d9-8117-324727c525f4",
   "metadata": {},
   "source": [
    "# 3. Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8692a38-7305-44f6-af16-72914830f16f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6. ì„ë² ë”© ëª¨ë¸ ì„ íƒ (í›ˆë ¨ ë°ì´í„°)\n",
    "embedding_model_name = RAG_Embedding_model\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2822c6-f7bd-4eda-83d1-380aaa4d9455",
   "metadata": {},
   "source": [
    "# 4. Vector DB\n",
    "## PDF -> FAISS\n",
    "## CSV -> Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a795a9-b3e6-4d95-b1ee-35812690f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "text_set = [chunk[\"text\"] for chunk in pdf_chunks] \n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (Type ì¶”ê°€)\n",
    "metadata_set = [\n",
    "    {\n",
    "        \"source\": chunk[\"source\"], \n",
    "        \"page_number\": chunk[\"page_number\"],\n",
    "    } \n",
    "    for chunk in pdf_chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150d47f-195c-4417-9ef5-2eb25822f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "metadata_set_CSV = [\n",
    "    {\n",
    "        \"ID\": row[\"ID\"],\n",
    "        \"ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)\": row[\"ê³µì‚¬ì¢…ë¥˜(ëŒ€ë¶„ë¥˜)\"],\n",
    "        \"ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)\": row[\"ê³µì‚¬ì¢…ë¥˜(ì¤‘ë¶„ë¥˜)\"],\n",
    "        \"ê³µì¢…(ëŒ€ë¶„ë¥˜)\": row[\"ê³µì¢…(ëŒ€ë¶„ë¥˜)\"],\n",
    "        \"ê³µì¢…(ì¤‘ë¶„ë¥˜)\": row[\"ê³µì¢…(ì¤‘ë¶„ë¥˜)\"],\n",
    "        \"ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)\": row[\"ì‚¬ê³ ê°ì²´(ëŒ€ë¶„ë¥˜)\"],\n",
    "        \"ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)\": row[\"ì‚¬ê³ ê°ì²´(ì¤‘ë¶„ë¥˜)\"]\n",
    "    }\n",
    "    for _, row in train.iterrows()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53af6e5-a359-4f57-8bdf-6b77402ff9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_set_CSVë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "df_metadata = pd.DataFrame(metadata_set_CSV)\n",
    "\n",
    "# NaN ê°’ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "nan_check = df_metadata.isna().any()\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ê° ì»¬ëŸ¼ì˜ NaN ì—¬ë¶€:\\n\", nan_check)\n",
    "\n",
    "# NaN ê°’ì´ ìˆëŠ” í–‰ ì¶”ì¶œ\n",
    "nan_rows = df_metadata[df_metadata.isna().any(axis=1)]\n",
    "\n",
    "# NaN ê°’ì´ ìˆëŠ” í–‰ì´ ìˆë‹¤ë©´ ì¶œë ¥\n",
    "if not nan_rows.empty:\n",
    "    print(\"NaN ê°’ì„ í¬í•¨í•œ í–‰:\\n\", nan_rows)\n",
    "else:\n",
    "    print(\"NaN ê°’ì´ í¬í•¨ëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e374f74-da6f-4ea4-9271-678522795245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b646c-dabc-4982-a7c5-b274d368f9dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "vectorstore = Chroma.from_texts(train_documents, embedding, metadatas=metadata_set_CSV)  \n",
    "\n",
    "# PDF\n",
    "vectorstore2 = FAISS.from_texts(text_set, embedding, metadatas=metadata_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0224c2a-b1a9-4298-924c-91daa3347f2a",
   "metadata": {},
   "source": [
    "# 5. Retriever (Ensemble Retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06aa5b-9f6a-49bf-96d1-ae3795388f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": R_k})\n",
    "\n",
    "# PDF\n",
    "retriever2 = vectorstore2.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": R_k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e96107-52cf-4ac0-aa67-ce2a224077bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[retriever, retriever2],\n",
    "    weights=Ensemble_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ffdf35-e583-4117-aae8-8dc4dd897a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LLM ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™”)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(LLM_Model, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078fbd3-b2fb-49a1-a532-556634286dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    do_sample=False,  # ê²°ì •ë¡ ì  ìƒì„±\n",
    "    return_full_text=False, \n",
    "    max_new_tokens=Max_token_length,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ê²½ê³ ë¥¼ ë¹„í™œì„±í™”í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3134fedc-dff0-42e6-a720-74661d2d7a02",
   "metadata": {},
   "source": [
    "# 6. Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c8018-9c11-4a89-9f37-0177d9c85b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Your mission is to be a safety expert answer bot. When given the {question}, you must provide an answer **only** if the question is related to **construction, safety, or safety measures**. \n",
    "\n",
    "If the question is **NOT** related to **construction, construction safety tasks, or construction safety plan tasks**, respond with:\n",
    "\n",
    "### ë‹µë³€ : \"ì—…ë¬´ì™€ ê´€ë ¨ì´ ì—†ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.\"\n",
    "### ë.\n",
    "\n",
    "Your answer **MUST** be in **only one sentence.\n",
    "Your answer **MUST** be written **only in Korean**.\n",
    "\n",
    "If any of the above instructions are violated, a penalty will be applied.\n",
    "\n",
    "Make sure to strictly follow these rules.\n",
    "\n",
    "### ì§ˆë¬¸:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=prompt_template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe3bf4-beda-4fec-9e55-f8187bfacd79",
   "metadata": {},
   "source": [
    "# 7. Reference Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649b70f-2a58-4168-a1cd-6e33d0c47cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "Sample_num = 5\n",
    "\n",
    "# combined_training_dataì—ì„œ ëœë¤í•˜ê²Œ 5ê°œ ìƒ˜í”Œ ì„ íƒ\n",
    "sample_questions = random.sample(list(combined_training_data['question']), Sample_num)\n",
    "\n",
    "# ê° ìƒ˜í”Œì— ëŒ€í•´ compression_retrieverì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ í™•ì¸\n",
    "for idx, question in enumerate(sample_questions):\n",
    "    print(f\"ìƒ˜í”Œ {idx+1}: {question}\\n\")\n",
    "\n",
    "    # ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    retrieved_docs = ensemble_retriever.get_relevant_documents(question)\n",
    "\n",
    "    print(\"ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:\")\n",
    "    for doc_idx, doc in enumerate(retrieved_docs):\n",
    "        print(f\"Document {doc_idx+1}: {doc.page_content}\")\n",
    "        print(f\"Meta Data : {doc.metadata}\\n\")  # ë©”íƒ€ë°ì´í„° ì¶œë ¥\n",
    "    \n",
    "    print(\"=\" * 100)  # ê°€ë…ì„±ì„ ìœ„í•œ êµ¬ë¶„ì„ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99caa754-5057-492b-b9dc-85abb445bbad",
   "metadata": {},
   "source": [
    "# 8. RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4e68b-f17a-433c-943f-38bfd35882b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659fb58-c456-4c5e-a12c-ff01fb7b46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": ensemble_retriever|format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# chain = (\n",
    "#     {\"context\": ensemble_retriever|format_docs, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdaee9-7bee-43db-a2bb-90874a9c3ad3",
   "metadata": {},
   "source": [
    "# 9. Visualization in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410dd183-0c98-4b3e-89be-645e77183ce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a7faa-0b0e-47cd-8812-c016b5b32626",
   "metadata": {},
   "source": [
    "# 10. Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c9cca-e92c-4d12-be80-053971ae1d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í†µí•© ìƒì„±\n",
    "combined_test_data = test.apply(\n",
    "    lambda row: {\n",
    "        \"question\": (\n",
    "            f\"ê¸´ê¸‰ìƒí™©ì…ë‹ˆë‹¤. ì‘ì—… í”„ë¡œì„¸ìŠ¤ëŠ” '{row['ì‘ì—…í”„ë¡œì„¸ìŠ¤']}'ì´ë©°, ì¸ì ì‚¬ê³ ëŠ” '{row['ì¸ì ì‚¬ê³ ']}' ê·¸ë¦¬ê³  ì‚¬ê³  ì›ì¸ì€ '{row['ì‚¬ê³ ì›ì¸']}'ì…ë‹ˆë‹¤. \"\n",
    "            f\"ì¬ë°œ ë°©ì§€ ëŒ€ì±… ë° í–¥í›„ ì¡°ì¹˜ ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "        )\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "combined_test_data = pd.DataFrame(list(combined_test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5921478-609c-435f-9344-ea07b5b6d38b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í›„ ê²°ê³¼ ì €ì¥\n",
    "test_results_t = []\n",
    "metadata_list_t = []  # ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "preview_count = 10\n",
    "\n",
    "# ë¯¸ë¦¬ ë³´ê¸° ìƒ˜í”Œ ìˆ˜\n",
    "preview_data = combined_test_data.sample(n=preview_count)\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘... ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜:\", preview_count)\n",
    "\n",
    "# tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "for idx, row in tqdm(preview_data.iterrows(), total=len(preview_data), desc=\"í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘\", unit=\"ìƒ˜í”Œ\"):\n",
    "    # RAG ì²´ì¸ í˜¸ì¶œ ë° ê²°ê³¼ ìƒì„±\n",
    "    prevention_result = chain.invoke(row['question'])\n",
    "\n",
    "    if isinstance(prevention_result, str):\n",
    "        result_text = prevention_result  # ë¬¸ìì—´ì´ë¼ë©´ ê·¸ ìì²´ë¥¼ ì‚¬ìš©\n",
    "    else:\n",
    "        result_text = prevention_result.get('result', '')\n",
    "\n",
    "    # ì§ˆë¬¸ê³¼ ê²°ê³¼ë¥¼ í•¨ê»˜ ì €ì¥\n",
    "    test_results_t.append({\"question\": row['question'], \"result\": result_text})\n",
    "\n",
    "    # ê°€ì¥ ë†’ì€ ìˆœìœ„ì˜ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘\n",
    "    retrieved_docs = ensemble_retriever.get_relevant_documents(row['question'])\n",
    "    \n",
    "    if retrieved_docs:\n",
    "        top_doc = retrieved_docs[0]  # ê°€ì¥ ë†’ì€ ìˆœìœ„ì˜ ë¬¸ì„œ\n",
    "\n",
    "        if \"source\" in top_doc.metadata:  # PDF ë¬¸ì„œì¼ ê²½ìš°\n",
    "            metadata_list_t.append({\n",
    "                \"Type\": 'pdf',\n",
    "                \"ID\": None,\n",
    "                \"Title\": top_doc.metadata[\"source\"],\n",
    "                \"Page_number\": top_doc.metadata[\"page_number\"]\n",
    "            })\n",
    "        elif \"ID\" in top_doc.metadata:  # CSV ë¬¸ì„œì¼ ê²½ìš°\n",
    "            metadata_list_t.append({\n",
    "                \"Type\": 'csv',\n",
    "                \"ID\": top_doc.metadata[\"ID\"],\n",
    "                \"Title\": None,\n",
    "                \"Page_number\": None\n",
    "            })\n",
    "\n",
    "print(\"\\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì™„ë£Œ! ì´ ê²°ê³¼ ìˆ˜:\", len(test_results_t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547de4f-2afc-4b4a-aa1a-f9fabf374590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "metadata_test = pd.DataFrame(metadata_list_t)\n",
    "metadata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56523aef-bc1e-4325-b11d-80ba192db179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ê° ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œ ë° ë‹µë³€ ì¶œë ¥\n",
    "for idx, result in enumerate(test_results_t):\n",
    "    question = result[\"question\"]\n",
    "    answer = result[\"result\"]  # ë‹µë³€ ê°€ì ¸ì˜¤ê¸°\n",
    "    print(f\"ì§ˆë¬¸ {idx + 1}: {question}\\n\")\n",
    "    print(f\"ë‹µë³€ {idx + 1}: {answer}\\n\")  # ë‹µë³€ ì¶œë ¥\n",
    "\n",
    "    # ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    retrieved_docs = ensemble_retriever.get_relevant_documents(question)\n",
    "\n",
    "    print(\"ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:\")\n",
    "    for doc_idx, doc in enumerate(retrieved_docs):\n",
    "        print(f\"Document {doc_idx + 1}: {doc.page_content}\")\n",
    "        print(f\"Meta Data: {doc.metadata}\\n\")  # ë©”íƒ€ë°ì´í„° ì¶œë ¥\n",
    "\n",
    "    print(\"=\" * 100)  # ê°€ë…ì„±ì„ ìœ„í•œ êµ¬ë¶„ì„ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9b884-0f38-41ec-943c-cb29b5034281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_results_të¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "results_df = pd.DataFrame(test_results_t)\n",
    "\n",
    "# ë‘ ë°ì´í„°í”„ë ˆì„ì„ í•©ì¹˜ê¸°\n",
    "combined_df = pd.concat([results_df, metadata_test], axis=1)\n",
    "combined_df['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4759ed-cf36-43f7-a107-dd8dfdd7102e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df['result'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c7d19-e7c1-469a-9a4f-d66a119915f1",
   "metadata": {},
   "source": [
    "# 11. PDF based Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b319e-9214-4aee-a3f4-e565615e33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸\n",
    "questions = [\n",
    "    \"í¬ë ˆì¸ ìš´ì „ì›ì´ ì§€ì¼œì•¼í•  ì•ˆì „ ìˆ˜ì¹™ì— ëŒ€í•´ ë§í•˜ì‹œì˜¤.\",\n",
    "    \"ê³¤ëŒë¼ ì‘ì—…ëŒ€ê°€ í—ˆìš© í•˜ê°• ì†ë„ë¥¼ ì´ˆê³¼í•˜ë©´ ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í•´ì•¼ í•˜ëŠ”ê°€?\",\n",
    "    \"ì•”ë°˜ ë³€í™” êµ¬ê°„ì˜ ë°œíŒŒ ì‘ì—…ì—ëŠ” ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ê°€?\",\n",
    "    \"ë¶€ì¬ì˜ í˜„ì¥ ë°˜ì… ì‹œ í•´ì•¼ë˜ëŠ” ì‘ì—…ì— ëŒ€í•´ì„œ ë§í•˜ì‹œì˜¤.\",\n",
    "    \"ë‹¨ìœ„ê³µì¢…ë³„ ì‘ì—… ì‹œì‘ ì „ í•´ì•¼ë˜ëŠ” ì¼ë“¤ì— ëŒ€í•´ ì„¤ëª…í•˜ì‹œì˜¤.\",\n",
    "    \"ì‹œìŠ¤í…œ ë™ë°”ë¦¬ ì¡°ë¦½ì‹œ ì‘ì—…ìê°€ í•´ì•¼ë˜ëŠ” ì—…ë¬´ì— ëŒ€í•´ ì„¤ëª…í•˜ì‹œì˜¤.\",\n",
    "    \"Uì ê±¸ì´ë¡œ ì‚¬ìš©í•´ì•¼ë˜ëŠ” ì•ˆì „ëŒ€ 'í˜¸' ë²ˆí˜¸ëŠ”?\",\n",
    "    \"ì•¼ê°„ ê·¼ë¡œìì˜ ì‘ì—… ì‹œê°„ì— ë”°ë¥¸ íœ´ì‹ ì‹œê°„ì€?\",\n",
    "    \"ë‚´ì¥ ê³µì‚¬ì—ì„œ ì‘ì—… ë°œíŒ ì‚¬ìš©ì‹œ í­ì˜ ê¸°ì¤€ì€?\",\n",
    "    \"ìºíŠ¸ì›Œí¬ê°€ ë°”ëŒì— í”ë“¤ë¦¬ëŠ” ê²½ìš°ì— í•´ì•¼ë˜ëŠ” ì¡°ì¹˜ëŠ”?\"\n",
    "]\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "PDF_question = pd.DataFrame({'question': questions})\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í›„ ê²°ê³¼ ì €ì¥\n",
    "PDF_question['Result'] = ''  # ê²°ê³¼ ì €ì¥ ì»¬ëŸ¼ ì¶”ê°€\n",
    "PDF_question['Type'] = ''  # ë¬¸ì„œ ìœ í˜•\n",
    "\n",
    "PDF_question['Doc_ID'] = None  # ë¬¸ì„œ ID (CSV)\n",
    "PDF_question['Title'] = ''  # ë¬¸ì„œ ì œëª© (PDF)\n",
    "PDF_question['Page_number'] = None  # í˜ì´ì§€ ë²ˆí˜¸ (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19e47f-be31-46e8-9926-90afe3a2e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "for idx, row in tqdm(PDF_question.iterrows(), total=len(PDF_question), desc=\"í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘\", unit=\"ìƒ˜í”Œ\"):\n",
    "    # RAG ì²´ì¸ í˜¸ì¶œ ë° ê²°ê³¼ ìƒì„±\n",
    "    prevention_result = chain.invoke(row['question'])\n",
    "    \n",
    "    if isinstance(prevention_result, str):\n",
    "        PDF_question.at[idx, 'Result'] = prevention_result  # ë¬¸ìì—´ ê²°ê³¼ ì €ì¥\n",
    "    else:\n",
    "        PDF_question.at[idx, 'Result'] = prevention_result.get('result', '')\n",
    "\n",
    "    # ê°€ì¥ ë†’ì€ ìˆœìœ„ì˜ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘\n",
    "    retrieved_docs = ensemble_retriever.get_relevant_documents(row['question'])\n",
    "    \n",
    "    if retrieved_docs:\n",
    "        top_doc = retrieved_docs[0]  # ê°€ì¥ ë†’ì€ ìˆœìœ„ì˜ ë¬¸ì„œ\n",
    "        metadata = top_doc.metadata\n",
    "\n",
    "        if \"source\" in metadata:  # PDF ë¬¸ì„œì¼ ê²½ìš°\n",
    "            PDF_question.at[idx, 'Type'] = 'pdf'\n",
    "            PDF_question.at[idx, 'Title'] = metadata[\"source\"]\n",
    "            PDF_question.at[idx, 'Page_number'] = metadata.get(\"page_number\", None)\n",
    "        elif \"ID\" in metadata:  # CSV ë¬¸ì„œì¼ ê²½ìš°\n",
    "            PDF_question.at[idx, 'Type'] = 'csv'\n",
    "            PDF_question.at[idx, 'Doc_ID'] = metadata[\"ID\"]\n",
    "\n",
    "print(\"\\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì™„ë£Œ! ì´ ê²°ê³¼ ìˆ˜:\", len(PDF_question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08c2f4-2066-47eb-b73d-47ae7079f670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PDF_question['Result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb54b139-ec5d-4d99-af60-e0cb81426442",
   "metadata": {},
   "source": [
    "# 12. Unrelated Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2bace-cc83-49a4-9440-954fac22fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ì—‰ëš±í•œ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸\n",
    "unrelated_questions = [\n",
    "    \"í•˜ëŠ˜ì€ ë¬´ìŠ¨ ìƒ‰ì¸ê°€ìš”?\",\n",
    "    \"í•œêµ­ì—ì„œ ê°€ì¥ ìœ ëª…í•œ ìŒì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ìš°ì£¼ì—ì„œëŠ” ì†Œë¦¬ê°€ ë“¤ë¦´ê¹Œìš”?\",\n",
    "    \"ê³ ì–‘ì´ëŠ” ì™œ ë°•ìŠ¤ë¥¼ ì¢‹ì•„í•˜ë‚˜ìš”?\",\n",
    "    \"ë¡œë§ˆ ì œêµ­ì´ ë©¸ë§í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ì¸ê°„ì˜ í‰ê·  ìˆ˜ë©´ ì‹œê°„ì€ ëª‡ ì‹œê°„ì¸ê°€ìš”?\",\n",
    "    \"ì´íƒˆë¦¬ì•„ì˜ ìˆ˜ë„ëŠ”?\",\n",
    "    \"í•œêµ­ì—ì„œ ê°€ì¥ ë†’ì€ ì‚°ì€ ì–´ë””ì¸ê°€ìš”?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6950ed-2e49-40cc-b12c-43374ede0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame ìƒì„±\n",
    "Unrelated_questions = pd.DataFrame({'question': unrelated_questions})\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í›„ ê²°ê³¼ ì €ì¥\n",
    "Unrelated_questions['Result'] = ''  # ê²°ê³¼ ì €ì¥ ì»¬ëŸ¼ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abc606-5be8-4db3-abef-448e20c6683a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "for idx, row in tqdm(Unrelated_questions.iterrows(), total=len(Unrelated_questions), desc=\"í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘\", unit=\"ìƒ˜í”Œ\"):\n",
    "    # RAG ì²´ì¸ í˜¸ì¶œ ë° ê²°ê³¼ ìƒì„±\n",
    "    prevention_result = chain.invoke(row['question'])\n",
    "\n",
    "    # prevention_resultì— \"ì—…ë¬´ì™€ ê´€ë ¨ì´ ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\"ê°€ ìˆìœ¼ë©´ ê·¸ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    if isinstance(prevention_result, str):\n",
    "        if \"ì—…ë¬´ì™€ ê´€ë ¨ì´\" in prevention_result:\n",
    "            Unrelated_questions.at[idx, 'Result'] = \"ì—…ë¬´ì™€ ê´€ë ¨ì´ ì—†ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.\"  # í•´ë‹¹ ë¬¸ì¥ìœ¼ë¡œ ë³€ê²½\n",
    "        else:\n",
    "            Unrelated_questions.at[idx, 'Result'] = prevention_result  # ë¬¸ìì—´ ê²°ê³¼ ì €ì¥\n",
    "    else:\n",
    "        result_text = prevention_result.get('result', '')\n",
    "        if \"ì—…ë¬´ì™€ ê´€ë ¨ì´\" in result_text:\n",
    "            Unrelated_questions.at[idx, 'Result'] = \"ì—…ë¬´ì™€ ê´€ë ¨ì´ ì—†ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.\"  # í•´ë‹¹ ë¬¸ì¥ìœ¼ë¡œ ë³€ê²½\n",
    "        else:\n",
    "            Unrelated_questions.at[idx, 'Result'] = result_text  # ê²°ê³¼ ì €ì¥\n",
    "\n",
    "    # ê°€ì¥ ë†’ì€ ìˆœìœ„ì˜ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘\n",
    "    retrieved_docs = ensemble_retriever.get_relevant_documents(row['question'])\n",
    "\n",
    "    if retrieved_docs:\n",
    "        top_doc = retrieved_docs[0]  # ê°€ì¥ ë†’ì€ ìˆœìœ„ì˜ ë¬¸ì„œ\n",
    "\n",
    "print(\"\\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì™„ë£Œ! ì´ ê²°ê³¼ ìˆ˜:\", len(Unrelated_questions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5a625-eb0b-4ea5-b79f-8a41cbdbdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Unrelated_questions['Result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e999a0-08e2-4f5e-a11c-9693d7c51c27",
   "metadata": {},
   "source": [
    "# 13. Inference & TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4fd82-b21b-4488-98dd-998efdba3852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥\n",
    "test_results = []\n",
    "metadata_list = []\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘... ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜:\", len(combined_test_data))\n",
    "\n",
    "# tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "for idx, row in tqdm(combined_test_data.iterrows(), total=len(combined_test_data), desc=\"í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘\", unit=\"ìƒ˜í”Œ\"):\n",
    "    # RAG ì²´ì¸ í˜¸ì¶œ ë° ê²°ê³¼ ìƒì„±\n",
    "    prevention_result = chain.invoke(row['question'])\n",
    "\n",
    "    if isinstance(prevention_result, str):\n",
    "        result_text = prevention_result\n",
    "    else:\n",
    "        result_text = prevention_result.get('result', '')\n",
    "\n",
    "    # 'ë‹µë³€' ì´ì „ ì œê±°\n",
    "    first_sentence = re.sub(r'.*ë‹µë³€\\s*[:ï¼š]?\\s*', '', result_text)\n",
    "    \n",
    "    # 'ë‹µë³€ :' ë˜ëŠ” 'ë‹µë³€:' ì œê±°\n",
    "    first_sentence = re.sub(r'ë‹µë³€\\s*[:ï¼š]?\\s*', '', first_sentence)\n",
    "\n",
    "    # '.'ì™€ ','ì„ ì œì™¸í•œ íŠ¹ìˆ˜ë¬¸ì ë° ì¤„ë°”ê¿ˆ ë¬¸ì ì œê±°\n",
    "    first_sentence = re.sub(r'[^ê°€-í£0-9\\s.,]', '', first_sentence).replace('\\n', '')\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ë¬¸ì¥ ëë‚˜ë©´ ê·¸ ë’¤ì˜ ë‚´ìš© ì œê±°\n",
    "    first_sentence = re.split(r'[.!?]\\s', first_sentence, maxsplit=1)[0].strip()\n",
    "\n",
    "\n",
    "    if \"ì—…ë¬´ì™€ ê´€ë ¨ì´\" in first_sentence:\n",
    "        first_sentence = \"ì—…ë¬´ì™€ ê´€ë ¨ì´ ì—†ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    test_results.append({\n",
    "        \"question\": row['question'],\n",
    "        \"result\": first_sentence\n",
    "    })\n",
    "\n",
    "    # ê°€ì¥ ë†’ì€ ìˆœìœ„ì˜ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘\n",
    "    retrieved_docs = ensemble_retriever.get_relevant_documents(row['question'])\n",
    "    \n",
    "    if retrieved_docs:\n",
    "        top_doc = retrieved_docs[0]  # ê°€ì¥ ë†’ì€ ìˆœìœ„ì˜ ë¬¸ì„œ\n",
    "        if \"source\" in top_doc.metadata:  # PDF ë¬¸ì„œì¼ ê²½ìš°\n",
    "            metadata_list.append({\n",
    "                \"Type\": 'pdf',\n",
    "                \"ID\": None,\n",
    "                \"Title\": top_doc.metadata.get(\"source\"),\n",
    "                \"Page_number\": top_doc.metadata.get(\"page_number\")\n",
    "            })\n",
    "        elif \"ID\" in top_doc.metadata:  # CSV ë¬¸ì„œì¼ ê²½ìš°\n",
    "            metadata_list.append({\n",
    "                \"Type\": 'csv',\n",
    "                \"ID\": top_doc.metadata.get(\"ID\"),\n",
    "                \"Title\": None,\n",
    "                \"Page_number\": None\n",
    "            })\n",
    "\n",
    "print(\"\\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì™„ë£Œ! ì´ ê²°ê³¼ ìˆ˜:\", len(test_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6247327-a403-4cdd-ac6b-122353252699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_model_name = TEST_Embedding\n",
    "embedding = SentenceTransformer(embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8172e1-1e5a-49b1-8b90-cdcd9d622abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc0782-b161-473f-a1a6-bf040b407563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ ì„ë² ë”© ìƒì„±\n",
    "# ë§Œì•½ test_resultsê°€ ë¦¬ìŠ¤íŠ¸ë¼ë©´\n",
    "if isinstance(test_results, list):\n",
    "    pred_embeddings = [embedding.encode(result['result']) for result in test_results]\n",
    "else:\n",
    "    pred_embeddings = embedding.encode(test_results['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4aed9e-180d-4448-94c0-f6fd625176a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e3e5a-3d58-4280-b255-bde05d8ee72f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_results[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f975a4-6946-40ab-8fce-3660c40c0134",
   "metadata": {},
   "source": [
    "# 14. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3f528-8581-4e4b-adb8-2a60ad345935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CSV íŒŒì¼ ì½ê¸°\n",
    "submission = pd.read_csv('./sample_submission.csv', encoding='utf-8-sig')\n",
    "# submissionì„ ë³µì‚¬í•˜ì—¬ submission2 ìƒì„±\n",
    "submission2 = submission.copy()\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "# test_resultsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°\n",
    "if isinstance(test_results, list):\n",
    "    results = [result['result'] for result in test_results]  # ê° resultë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "else:\n",
    "    results = test_results['result']  # ë‹¨ì¼ ê²°ê³¼ì¸ ê²½ìš°\n",
    "\n",
    "# ê²°ê³¼ë¥¼ submissionì— í• ë‹¹\n",
    "submission.iloc[:, 1] = results  # resultsë¥¼ submissionì˜ ë‘ ë²ˆì§¸ ì—´ì— í• ë‹¹\n",
    "\n",
    "# pred_embeddingsê°€ 2ì°¨ì› ë°°ì—´ì¸ ê²½ìš°\n",
    "submission.iloc[:, 2:] = pred_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb739c-9f0f-4955-ab95-f2fecbca9db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a70ea4-55cb-4209-8029-2335673eb28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥\n",
    "submission.to_csv('./RAG_Ensemble_final_5.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5655100-66c8-4cac-9d9f-da47b7017c29",
   "metadata": {},
   "source": [
    "# 15. Add File for Qualitative Evaluation\n",
    "### 1) Train : ID\n",
    "### 2) PDF : Source(PDF Title), Page Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e89b1-5f27-4f98-93a9-1515aa8a7525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "metadata = pd.DataFrame(metadata_list)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3f531-395a-4d71-bd88-0e078435c2d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_results_të¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "results_set = pd.DataFrame(test_results)\n",
    "\n",
    "# ë‘ ë°ì´í„°í”„ë ˆì„ì„ í•©ì¹˜ê¸°\n",
    "Q_Evaluation = pd.concat([results_set, metadata], axis=1)\n",
    "Q_Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1d12d-48b1-4bfb-bb98-d0af24da0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€í‘œ\n",
    "Q_Evaluation.to_csv('./Evaluation_metric_final5.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c738d9-4226-4669-8c9c-31119109d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab0d30-b4ca-4b2a-926e-e98acf915478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Type\" ì»¬ëŸ¼ì˜ ê³ ìœ í•œ ìš”ì†Œ ì¶œë ¥\n",
    "unique_types = combined_df['Type'].unique()\n",
    "\n",
    "print(\"Type ì»¬ëŸ¼ì˜ ê³ ìœ í•œ ìš”ì†Œ:\")\n",
    "for t in unique_types:\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c868b7-e921-4eee-9ba5-629dc651f14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e3091-539b-433c-92dc-2c69ac742500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
